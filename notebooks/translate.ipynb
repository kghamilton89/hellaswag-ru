{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29467e94",
   "metadata": {},
   "source": [
    "# Translating the HellaSwag Dataset to Russian\n",
    "\n",
    "The goal of this notebook is to translate the [HellaSwag dataset](https://huggingface.co/datasets/Rowan/hellaswag) to Russian. See [README.md](../README.md) for more information about the HellaSwag benchmark, and how it can be used to assess common-sense sentence-completion in Large Language Models (LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065288de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# load initial libs\n",
    "%pip install datasets huggingface_hub[hf_xet] ipywidgets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9bc545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the dataset from hugging face hub\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Rowan/hellaswag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a90e5daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (ctx): Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. then\n",
      "\n",
      "Endings: [', the man adds wax to the windshield and cuts it.', ', a person board a ski lift, while two men supporting the head of the person wearing winter clothes snow as the we girls sled.', ', the man puts on a christmas coat, knitted with netting.', ', the man continues removing the snow on his car.']\n",
      "\n",
      "Label (correct ending index): 3\n"
     ]
    }
   ],
   "source": [
    "# cool to see that hellaswag already supports the new, more efficient xet file storage\n",
    "# read more about xet here: https://huggingface.co/blog/xet-on-the-hub\n",
    "\n",
    "# let's have a look at a few sample from the hellaswag dataset\n",
    "sample = dataset[\"train\"][0]\n",
    "print(\"Context (ctx):\", sample[\"ctx\"])\n",
    "print(\"\\nEndings:\", sample[\"endings\"])\n",
    "print(\"\\nLabel (correct ending index):\", sample[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb9bf434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: {'ind': Value(dtype='int32', id=None), 'activity_label': Value(dtype='string', id=None), 'ctx_a': Value(dtype='string', id=None), 'ctx_b': Value(dtype='string', id=None), 'ctx': Value(dtype='string', id=None), 'endings': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'source_id': Value(dtype='string', id=None), 'split': Value(dtype='string', id=None), 'split_type': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None)}\n",
      "Train samples: 39905\n",
      "Validation samples: 10042\n",
      "Test samples: 10003\n"
     ]
    }
   ],
   "source": [
    "# print out the features of the dataset\n",
    "print(\"Features:\", dataset[\"train\"].features)\n",
    "\n",
    "# check number of rows for each split\n",
    "print(\"Train samples:\", len(dataset[\"train\"]))\n",
    "print(\"Validation samples:\", len(dataset[\"validation\"]))\n",
    "print(\"Test samples:\", len(dataset[\"test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d0ed85",
   "metadata": {},
   "source": [
    "## Using Open Source Translation\n",
    "\n",
    "For this exercise, we'll use the [Opus MT](https://huggingface.co/Helsinki-NLP/opus-mt-en-ru) translation model from the [University of Helsinki NLP department](https://huggingface.co/Helsinki-NLP).\n",
    "\n",
    "Once we've translated the data, we'll revisit it with human annotators to confirm its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d73f68f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install additional libs\n",
    "%pip install transformers sentencepiece sacremoses --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc1ab2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(62518, 512, padding_idx=62517)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(62518, 512, padding_idx=62517)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(62518, 512, padding_idx=62517)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=62518, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "# load model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-ru\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# use gpu if possible (this increases the speed of translation)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3be3c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to translate our english-language dataset and component elements into russian language\n",
    "def translate_texts(texts, tokenizer, model, max_length=512):\n",
    "    \"\"\"Batch-translate a list of texts from English to Russian\"\"\"\n",
    "    batch = tokenizer.prepare_seq2seq_batch(\n",
    "        src_texts=texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    ).to(device)\n",
    "\n",
    "    translated = model.generate(**batch)\n",
    "    return tokenizer.batch_decode(translated, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e16f9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def translate_texts_parallel(ctx_batch, endings_batch, tokenizer, model):\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        future_ctx = executor.submit(translate_texts, ctx_batch, tokenizer, model)\n",
    "        flat_endings = sum(endings_batch, [])\n",
    "        future_endings = executor.submit(translate_texts, flat_endings, tokenizer, model)\n",
    "        return future_ctx.result(), future_endings.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e85fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def save_jsonl(path, data):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7b1a44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4106: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ctx:\n",
      " Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. then\n",
      "\n",
      "Translated ctx:\n",
      " –ó–∞—Ç–µ–º –º—É–∂—á–∏–Ω–∞ –ø–∏—à–µ—Ç –ø–æ —Å–Ω–µ–≥—É, –∑–∞–∫—Ä—ã–≤–∞—è –æ–∫–Ω–æ –º–∞—à–∏–Ω—ã, –∏ –∂–µ–Ω—â–∏–Ω–∞ –≤ –∑–∏–º–Ω–µ–π –æ–¥–µ–∂–¥–µ —É–ª—ã–±–∞–µ—Ç—Å—è.\n",
      "\n",
      "Original endings:\n",
      " [', the man adds wax to the windshield and cuts it.', ', a person board a ski lift, while two men supporting the head of the person wearing winter clothes snow as the we girls sled.', ', the man puts on a christmas coat, knitted with netting.', ', the man continues removing the snow on his car.']\n",
      "\n",
      "Translated endings:\n",
      " ['–ß–µ–ª–æ–≤–µ–∫ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤–æ—Å–∫ –∫ –ª–æ–±–æ–≤–æ–º—É —Å—Ç–µ–∫–ª—É –∏ —Ä–∞–∑—Ä–µ–∑–∞–µ—Ç –µ–≥–æ.', ', —á–µ–ª–æ–≤–µ–∫ —Å–∏–¥–∏—Ç –Ω–∞ –ª—ã–∂–Ω–æ–º –ø–æ–¥—ä–µ–º–Ω–∏–∫–µ, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –¥–≤–æ–µ –º—É–∂—á–∏–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –≥–æ–ª–æ–≤—É —á–µ–ª–æ–≤–µ–∫–∞ –≤ –∑–∏–º–Ω–µ–º —Å–Ω–µ–≥—É, –∫–∞–∫ –º—ã, –¥–µ–≤–æ—á–∫–∏, —Å–∞–ø–ª–∏–≤–∞–ª–∏.', '–ú—É–∂—á–∏–Ω–∞ –Ω–∞–¥–µ–≤–∞–µ—Ç —Ä–æ–∂–¥–µ—Å—Ç–≤–µ–Ω—Å–∫–æ–µ –ø–∞–ª—å—Ç–æ, –≤—è–∑–∞–Ω–Ω–æ–µ —Å–µ—Ç–∫–æ–π.', '–ú—É–∂—á–∏–Ω–∞ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —É–¥–∞–ª—è—Ç—å —Å–Ω–µ–≥ –Ω–∞ —Å–≤–æ–µ–π –º–∞—à–∏–Ω–µ.']\n"
     ]
    }
   ],
   "source": [
    "# let's test our function on a single sample\n",
    "sample = dataset[\"train\"][0]\n",
    "\n",
    "# translate context\n",
    "translated_ctx = translate_texts([sample[\"ctx\"]], tokenizer, model)[0]\n",
    "\n",
    "# translate all possible endings\n",
    "translated_endings = translate_texts(sample[\"endings\"], tokenizer, model)\n",
    "\n",
    "# show results\n",
    "print(\"Original ctx:\\n\", sample[\"ctx\"])\n",
    "print(\"\\nTranslated ctx:\\n\", translated_ctx)\n",
    "print(\"\\nOriginal endings:\\n\", sample[\"endings\"])\n",
    "print(\"\\nTranslated endings:\\n\", translated_endings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da623a34",
   "metadata": {},
   "source": [
    "## Translating the Full Dataset\n",
    "\n",
    "We've written a function to programmatically translate the HellaSwag dataset and confirmed that it is working to an acceptable standard. \n",
    "\n",
    "Let's iterate through the entire dataset now, using batch processing to avoid rates limits from Hugging Face, and create a new Russian-language HellaSwag dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b85d382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install additional libs\n",
    "%pip install tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195510a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m–ü—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∫–æ–¥–∞ –≤ —Ç–µ–∫—É—â–µ–π —è—á–µ–π–∫–µ –∏–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —è—á–µ–π–∫–µ —è–¥—Ä–æ –∞–≤–∞—Ä–∏–π–Ω–æ –∑–∞–≤–µ—Ä—à–∏–ª–æ —Ä–∞–±–æ—Ç—É. \n",
      "\u001b[1;31m–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–¥ –≤ —è—á–µ–π–∫–∞—Ö, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω—É—é –ø—Ä–∏—á–∏–Ω—É —Å–±–æ—è. \n",
      "\u001b[1;31m–©–µ–ª–∫–Ω–∏—Ç–µ <a href='https://aka.ms/vscodeJupyterKernelCrash'>–∑–¥–µ—Å—å</a>, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–≤–µ–¥–µ–Ω–∏—è. \n",
      "\u001b[1;31m–ü–æ–¥—Ä–æ–±–Ω–µ–µ —Å–º. –≤ <a href='command:jupyter.viewOutput'>–∂—É—Ä–Ω–∞–ª–µ Jupyter</a>."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def batched_translate_with_checkpoint(dataset_split, split_name, output_dir=\"checkpoints\", batch_size=4):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    checkpoint_path = Path(output_dir) / f\"{split_name}.jsonl\"\n",
    "\n",
    "    translated_data = load_jsonl(checkpoint_path) if checkpoint_path.exists() else []\n",
    "    start_index = len(translated_data)\n",
    "\n",
    "    print(f\"üìù Resuming {split_name} from index {start_index}...\")\n",
    "\n",
    "    for i in tqdm(range(start_index, len(dataset_split), batch_size), desc=f\"Translating {split_name}\"):\n",
    "        batch = dataset_split.select(range(i, min(i + batch_size, len(dataset_split))))\n",
    "        batch_dicts = batch.to_list()\n",
    "\n",
    "        ctx_batch = [ex[\"ctx\"] for ex in batch_dicts]\n",
    "        endings_batch = [ex[\"endings\"] for ex in batch_dicts]\n",
    "\n",
    "        try:\n",
    "            translated_ctx, translated_flat_endings = translate_texts_parallel(ctx_batch, endings_batch, tokenizer, model)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error on batch {i}-{i+batch_size}: {e}\")\n",
    "            continue\n",
    "\n",
    "        translated_endings = [\n",
    "            translated_flat_endings[j:j+4] for j in range(0, len(translated_flat_endings), 4)\n",
    "        ]\n",
    "\n",
    "        for j in range(len(batch_dicts)):\n",
    "            translated_entry = {\n",
    "                \"ctx\": translated_ctx[j],\n",
    "                \"endings\": translated_endings[j],\n",
    "                \"label\": batch_dicts[j].get(\"label\", -1)\n",
    "            }\n",
    "            translated_data.append(translated_entry)\n",
    "\n",
    "            with open(checkpoint_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(translated_entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Done: {split_name} ‚Äî {len(translated_data)} total entries\")\n",
    "    return translated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2da991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Resuming train from index 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating train:   2%|‚ñè         | 186/9977 [33:56<22:49:41,  8.39s/it]"
     ]
    }
   ],
   "source": [
    "translated_train = batched_translate_with_checkpoint(dataset[\"train\"], \"train\")\n",
    "translated_val = batched_translate_with_checkpoint(dataset[\"validation\"], \"validation\")\n",
    "\n",
    "translated_test = None\n",
    "if \"test\" in dataset:\n",
    "    translated_test = batched_translate_with_checkpoint(dataset[\"test\"], \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load from saved files (ensures full resumption)\n",
    "train_data = load_jsonl(\"checkpoints/train.jsonl\")\n",
    "val_data = load_jsonl(\"checkpoints/validation.jsonl\")\n",
    "test_data = load_jsonl(\"checkpoints/test.jsonl\") if Path(\"checkpoints/test.jsonl\").exists() else None\n",
    "\n",
    "hf_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_data),\n",
    "    \"validation\": Dataset.from_list(val_data),\n",
    "})\n",
    "\n",
    "if test_data:\n",
    "    hf_dataset[\"test\"] = Dataset.from_list(test_data)\n",
    "\n",
    "hf_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload resulting dataset to hugging face hub\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "hf_dataset.push_to_hub(\"ZennyKenny/hellaswag-ru\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
